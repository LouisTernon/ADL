{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-classification problem\n",
    "\n",
    "__Note:__ you might need to do\n",
    "`conda install torchvision \"pillow<7\"`\n",
    "if torchvision is not already installed on your computer, and/or for compatibility issues (the version of torchvision version supporting the last version of Pillow is not released yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "%matplotlib inline\n",
    "import time\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./TP2_results\"):\n",
    "    os.makedirs(\"TP2_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [MNIST Dataset](https://pytorch.org/docs/stable/torchvision/datasets.html#mnist)\n",
    "* Handwritten digits with 10 classes\n",
    "* the size of each image is 28x28 pixels \n",
    "* 50 000 data examples in training set, 10 000 examples in validation set, 10 000 in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading MNIST dataset from torchvision.dataset\n",
    "dataset = torchvision.datasets.MNIST(root='data/',\n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the dataset is : torch.Size([60000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(\"The shape of the dataset is :\", dataset.data.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset to training and validation sets\n",
    "train_set, val_set = random_split(dataset, [50000, 10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'image label: 0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_index = 88\n",
    "\n",
    "plt.imshow(dataset.data[sample_index], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.title(\"image label: %d\" % dataset.targets[sample_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.l1 = nn.Linear(28*28, 10)# Input size is 28*28, output size should be the same with the number of classes\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        outputs = F.softmax(self.l1(inputs), dim=1)# Use softmax as the activation function for the last layer\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model: \n",
    "model = Model()\n",
    "\n",
    "# Choose the hyperparameters for training: \n",
    "num_epochs = 10\n",
    "batch_size = 10\n",
    "\n",
    "# Use mean squared loss function \n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Use SGD optimizer with a learning rate of 0.01\n",
    "# It is initialized on our model\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function for training\n",
    "def train(num_epochs, batch_size, criterion, optimizer, model, dataset, model_type=\"linear\", one_hot=True):\n",
    "    train_error = []\n",
    "    train_loader = DataLoader(dataset, batch_size, shuffle=True)\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_average_loss = 0.0\n",
    "        for (images, labels) in train_loader:\n",
    "            if model_type==\"linear\": y_pre = model(images.view(batch_size, -1)) \n",
    "            # reshape the inputs from [N, img_shape, img_shape] to [N, img_shape*img_shape] \n",
    "            elif model_type==\"conv\": y_pre = model(images) #y_pre = model(images.unsqueeze_(1)) \n",
    "            else: raise NotImplemented\n",
    "            # One-hot encoding or labels so as to calculate MSE error:\n",
    "            if one_hot:\n",
    "                labels_one_hot = torch.FloatTensor(batch_size, 10)\n",
    "                labels_one_hot.zero_()\n",
    "                labels_one_hot.scatter_(1, labels.view(-1, 1), 1)\n",
    "                labels = labels_one_hot\n",
    "           \n",
    "            loss = criterion(y_pre, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_average_loss += loss.item() * batch_size / len(dataset)\n",
    "        train_error.append(epoch_average_loss)\n",
    "        print('Epoch [{}/{}], Loss: {:.4f}'\n",
    "                      .format(epoch+1, num_epochs, epoch_average_loss))\n",
    "    return train_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.0756\n",
      "Epoch [2/10], Loss: 0.0484\n",
      "Epoch [3/10], Loss: 0.0364\n",
      "Epoch [4/10], Loss: 0.0305\n",
      "Epoch [5/10], Loss: 0.0272\n",
      "Epoch [6/10], Loss: 0.0251\n",
      "Epoch [7/10], Loss: 0.0236\n",
      "Epoch [8/10], Loss: 0.0225\n",
      "Epoch [9/10], Loss: 0.0216\n",
      "Epoch [10/10], Loss: 0.0209\n"
     ]
    }
   ],
   "source": [
    "train_error = train(num_epochs, batch_size, criterion, optimizer, model, train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Visualization of convergence')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the training error wrt. the number of epochs: \n",
    "plt.plot(range(1, num_epochs+1), train_error)\n",
    "plt.xlabel(\"num_epochs\")\n",
    "plt.ylabel(\"Train error\")\n",
    "plt.title(\"Visualization of convergence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Model on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy to evaluate the model\n",
    "def accuracy(dataset, model, model_type=\"linear\"):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        dataloader = DataLoader(dataset)\n",
    "        for images, labels in dataloader:\n",
    "            if model_type==\"linear\": images = images.view(-1, 28*28)\n",
    "            else:\n",
    "                pass\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1) \n",
    "            correct += (predicted == labels).sum()\n",
    "    acc = 100*correct.item()/ len(dataset)\n",
    "    print('Accuracy of the model : {:.2f} %'.format(acc))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model : 88.29 %\n"
     ]
    }
   ],
   "source": [
    "accuracy(val_set, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Prediction label: 7')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_index = 66\n",
    "\n",
    "(image, label) = val_set[val_index]\n",
    "output = model(image.view(-1, 28*28))\n",
    "_, prediction = torch.max(output.data, 1)\n",
    "\n",
    "plt.imshow(image.view(28, 28), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.title(\"Prediction label: %d\" % prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Impact of the architecture of the model\n",
    "Define your own class `Model` to improve the predictions:\n",
    "\n",
    "* The convolutional layer can be a good choice to deal with images. Replace nn.Linear with [nn.Conv2d](https://pytorch.org/docs/stable/nn.html#conv2d).\n",
    "* Try to add more layers (1, 2, 3, more ?)\n",
    "* Change the number of neurons in hidden layers (5, 10, 20, more ?)\n",
    "* Try different activation functions such as [sigmoid](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.sigmoid), [tanh](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.tanh), [relu](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.relu), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(in_channels, out_channels, kernel_size, activation, *args, **kwargs):\n",
    "    block = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size, *args, **kwargs), activation)\n",
    "    return block\n",
    "\n",
    "def linear_block(in_features, out_features, activation, *args, **kwargs):\n",
    "    block = nn.Sequential(nn.Linear(in_features, out_features), activation)\n",
    "    return block\n",
    "\n",
    "class ConvModel(nn.Module):\n",
    "\n",
    "    def __init__(self, activation:str, n_filters:list, n_neurons:list, in_channels=1, in_dim=28*28, out_dim=10, logits=False):\n",
    "        super(ConvModel, self).__init__()\n",
    "\n",
    "        activations = nn.ModuleDict([[\"relu\", nn.ReLU()], [\"lrelu\", nn.LeakyReLU()], [\"sigmoid\", nn.Sigmoid()], [\"tanh\", nn.Tanh()]])\n",
    "\n",
    "        self.conv_space_dim = [in_channels, *n_filters]\n",
    "        self.conv_blocks = [conv_block(in_channels, out_channels, kernel_size=3, activation=activations[activation], padding=1)\n",
    "        for in_channels, out_channels in zip(self.conv_space_dim, self.conv_space_dim[1:])]\n",
    "        self.encoder = nn.Sequential(*self.conv_blocks)\n",
    "\n",
    "        self.linear_space_dim = [self.conv_space_dim[-1]*in_dim, *n_neurons]\n",
    "        self.linear_blocks = [linear_block(in_features, out_features, activations[activation]) for in_features, out_features in zip(self.linear_space_dim, self.linear_space_dim[1:])]\n",
    "        self.fully_connected = nn.Sequential(*self.linear_blocks, nn.Linear(self.linear_space_dim[-1], out_dim))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        #outputs = F.softmax(self.encoder(inputs), dim=1)# Use softmax as the activation function for the last layer\n",
    "        x = self.encoder(inputs)\n",
    "        x = x.view(x.size(0), -1) # Flatten\n",
    "        outputs = self.fully_connected(x)\n",
    "        if not logits: outputs = F.softmax(x, dim=1)\n",
    "    \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = [\"relu\", \"lrelu\", \"sigmoid\", \"tanh\"]\n",
    "n_filters = [4, 8, 16, 32]\n",
    "conv_layers = [0, 1, 2, 3]\n",
    "fc_layers = [0, 1, 2]\n",
    "n_neurons = [10, 15, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Training model with filters : [8, 8], activation : relu, and neurons: [10]\n",
      "Epoch [1/2], Loss: 0.0202\n",
      "Epoch [2/2], Loss: 0.0083\n",
      "Accuracy of the model : 95.22 %\n",
      "Grid evaluation took 56.09870409965515 sec\n"
     ]
    }
   ],
   "source": [
    "results = {\"filters\": [], \"neurons\": [], \"activation\":[], \"acc\":[]}\n",
    "start_time = time.time()\n",
    "for filters in [[8, 8]]:\n",
    "    for neurons in [[10]]:\n",
    "        for activation in [\"relu\"]:\n",
    "            print(f\"------- Training model with filters : {filters}, activation : {activation}, and neurons: {neurons}\")\n",
    "            # Init model \n",
    "            model = ConvModel(activation=activation, n_filters=filters, n_neurons=neurons)\n",
    "            criterion = nn.MSELoss()\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "            train_error = train(num_epochs, batch_size, criterion, optimizer, model, train_set, model_type='conv')\n",
    "\n",
    "            acc = accuracy(val_set, model, model_type=\"conv\")\n",
    "\n",
    "            curr_res = {\"filters\": n_filters, \"activation\":activation, \"neurons\":neurons, \"acc\":acc}\n",
    "            [results[key].append(val) for key, val in curr_res.items()]\n",
    "\n",
    "print(f\"Grid evaluation took {time.time()-start_time} sec\")         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame.from_dict(results)\n",
    "df_results.to_csv(os.path.join(\"TP2_results\", \"ex1_grid_search.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Impact of the optimizer\n",
    "Retrain the model by using different parameters of the optimizer; you can change its parameters in the cell initializing it, after the definition of your model.\n",
    "\n",
    "* Use different batch sizes, from 10 to 1 000 for instance\n",
    "* Try different values of the learning rate (between 0.001 and 10), and see how these impact the training process. Do all network architectures react the same way to different learning rates?\n",
    "* Change the duration of the training by increasing the number of epochs\n",
    "* Try other optimizers, such as [Adam](https://pytorch.org/docs/stable/optim.html?highlight=adam#torch.optim.Adam) or [RMSprop](https://pytorch.org/docs/stable/optim.html?highlight=rmsprop#torch.optim.RMSprop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters, neurons, activation = [6, 6], [], \"relu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_list = [int(x) for x in np.linspace(10, 400, 5)]\n",
    "lr_list = np.logspace(-3, 1, 5)\n",
    "optimizer_dict = {\"Adagrad\": torch.optim.Adagrad, \"Adam\": torch.optim.Adam, \"RMSprop\": torch.optim.RMSprop, \"SGD\": torch.optim.SGD}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with batch_size: 10, lr : 0.001, optimizer : Adagrad\n",
      "Epoch [1/2], Loss: 0.0239\n",
      "Epoch [2/2], Loss: 0.0149\n",
      "Accuracy of the model : 90.76 %\n",
      "Grid evaluation took 51.23738217353821 sec\n"
     ]
    }
   ],
   "source": [
    "results = {\"batch_size\": [], \"lr\":[], \"optimizer\":[], \"acc\":[]}\n",
    "start_time = time.time()\n",
    "for batch_size in batch_size_list[:1]:\n",
    "    for lr in lr_list[:1]: \n",
    "        for optim_name, optimizer in list(optimizer_dict.items())[:1]:     \n",
    "            print(f\"Training model with batch_size: {batch_size}, lr : {lr}, optimizer : {optim_name}\")\n",
    "            model = ConvModel(activation=activation, n_filters=filters, n_neurons=neurons)\n",
    "            criterion = nn.MSELoss()\n",
    "            optimizer = optimizer(model.parameters(), lr=lr)\n",
    "            train_error = train(num_epochs, batch_size, criterion, optimizer, model, train_set, model_type='conv')\n",
    "\n",
    "            acc = accuracy(val_set, model, model_type=\"conv\")\n",
    "\n",
    "            curr_res = {\"batch_size\": batch_size, \"lr\":lr, \"optimizer\":optim_name, \"acc\":acc}\n",
    "            [results[key].append(val) for key, val in curr_res.items()]\n",
    "            \n",
    "print(f\"Grid evaluation took {time.time()-start_time} sec\")         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame.from_dict(results)\n",
    "df_results.to_csv(os.path.join(\"TP2_results\", \"ex2_grid_search.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Impact of the loss function\n",
    "The MSE error is rarely used in this case. The cross entropy loss can be a better choice for multi-classification problems. In pytorch, the cross entropy loss is defined by [nn.CrossEntropyLoss](https://pytorch.org/docs/stable/nn.html#crossentropyloss). Replace the MSE loss by this one to observe its impact.\n",
    "\n",
    "**Note:** In order to use nn.CrossEntropyLoss correctly, don't add an activation function to the last layer of your network. And one-hot encoding is no longer needed to calculate the loss, delete the encoding procedures in function `train`.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation, filters, neurons, optimizer, lr, batch_size = \"relu\", [6, 6], [], torch.optim.Adam, 1e-3, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_dict = {\"MSELoss\": nn.MSELoss, \"BCELoss\": nn.BCELoss, \"BCEWithLogitsLoss\": nn.BCEWithLogitsLoss}\n",
    "criterion_w_logits = [\"BCEWithLogitsLoss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with criterion: MSELoss\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "bool value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-157-5b2b48bbd60c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mone_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcriterion_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcriterion_w_logits\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain_error\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'conv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"conv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-154-f78086a403c0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_epochs, batch_size, criterion, optimizer, model, dataset, model_type, one_hot)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pre\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/torch/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/torch/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_Loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/torch/lib/python3.6/site-packages/torch/nn/_reduction.py\u001b[0m in \u001b[0;36mlegacy_get_string\u001b[0;34m(size_average, reduce, emit_warning)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mreduce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mean'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: bool value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "results = {\"criterion\": [], \"acc\":[], \"num_epochs\":[]}\n",
    "start_time = time.time()\n",
    "for criterion_name, criterion in criterion_dict.items():     \n",
    "    print(f\"Training model with criterion: {criterion_name}\")\n",
    "    logits = True if criterion_name in criterion_w_logits else False\n",
    "    model = ConvModel(activation=activation, n_filters=filters, n_neurons=neurons, logits=logits)\n",
    "    _criterion = criterion()\n",
    "    _optimizer = optimizer(model.parameters(), lr=lr)\n",
    "    \n",
    "    one_hot = False if criterion_name in criterion_w_logits else True\n",
    "    train_error =  train(num_epochs, batch_size, criterion, optimizer, model, train_set, model_type='conv', one_hot=one_hot)\n",
    "\n",
    "    acc = accuracy(val_set, model, model_type=\"conv\")\n",
    "    \n",
    "    curr_res = {\"criterion\": criterion_name, \"acc\":acc}\n",
    "    [results[key].append(val) for key, val in curr_res.items()]\n",
    "\n",
    "print(f\"Grid evaluation took {time.time()-start_time} sec\")         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame.from_dict(results)\n",
    "df_results.to_csv(os.path.join(\"TP2_results\", \"ex3_grid_search.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Prediction on test set\n",
    "\n",
    "Once you have a model that seems satisfying on the validation dataset, you SHOULD evaluate it on a test dataset that has never been used before, to obtain a final accuracy value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading MNIST test set from torchvision.dataset\n",
    "test_set = torchvision.datasets.MNIST(root='data/',\n",
    "                                         train=False,\n",
    "                                         transform=transforms.ToTensor(),\n",
    "                                         download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(test_set, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
